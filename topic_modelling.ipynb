{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Ravi Varma Injeti',\n",
       " 'C:\\\\Users\\\\Ravi Varma Injeti\\\\anaconda3\\\\python37.zip',\n",
       " 'C:\\\\Users\\\\Ravi Varma Injeti\\\\anaconda3\\\\DLLs',\n",
       " 'C:\\\\Users\\\\Ravi Varma Injeti\\\\anaconda3\\\\lib',\n",
       " 'C:\\\\Users\\\\Ravi Varma Injeti\\\\anaconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\Ravi Varma Injeti\\\\anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\Users\\\\Ravi Varma Injeti\\\\anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\Users\\\\Ravi Varma Injeti\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\Users\\\\Ravi Varma Injeti\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin',\n",
       " 'C:\\\\Users\\\\Ravi Varma Injeti\\\\anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions',\n",
       " 'C:\\\\Users\\\\Ravi Varma Injeti\\\\.ipython']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"E:/Topic Modelling/\"\n",
    "raw_data = pd.read_csv(file_path + 'abcnews-date-text.csv')\n",
    "raw_data.columns\n",
    "\n",
    "\n",
    "raw_data_text = raw_data[:300000][['headline_text']];\n",
    "raw_data_text['index'] =raw_data_text.index\n",
    "documents = raw_data_text\n",
    "\n",
    "print(len(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Ravi Varma\n",
      "[nltk_data]     Injeti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos = 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##function for lemarizing a each test using wornet lematizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos = 'v'))\n",
    "    #return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocessing_text(text):\n",
    "    results = []\n",
    "    for tokens in gensim.utils.simple_preprocess(text):\n",
    "        if tokens not in gensim.parsing.preprocessing.STOPWORDS and len(tokens)>3:\n",
    "            stem = lemmatize_stemming(tokens)\n",
    "            results.append(stem)\n",
    "    return results\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_text:  seaman injury crisis for arsenal\n",
      "['seaman', 'injury', 'crisis', 'for', 'arsenal']\n",
      "pre_processed_text:  ['seaman', 'injuri', 'crisi', 'arsenal']\n"
     ]
    }
   ],
   "source": [
    "document_num = 402\n",
    "raw_text = documents[documents['index'] == document_num].values[0][0]\n",
    "\n",
    "print(\"raw_text: \", raw_text)\n",
    "\n",
    "words=[]\n",
    "for word in raw_text.split():\n",
    "    words.append(word)\n",
    "\n",
    "print(words)    \n",
    "print(\"pre_processed_text: \", preprocessing_text(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>commuters evacuated from circular quay train</td>\n",
       "      <td>299995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>companies invest billions in png gas industry</td>\n",
       "      <td>299996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>company looks to expand basalt quarry</td>\n",
       "      <td>299997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>cooloola mayor flags support for super council</td>\n",
       "      <td>299998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>cosgrove relieved vcs staying in aust</td>\n",
       "      <td>299999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline_text   index\n",
       "0       aba decides against community broadcasting lic...       0\n",
       "1          act fire witnesses must be aware of defamation       1\n",
       "2          a g calls for infrastructure protection summit       2\n",
       "3                air nz staff in aust strike for pay rise       3\n",
       "4           air nz strike to affect australian travellers       4\n",
       "...                                                   ...     ...\n",
       "299995       commuters evacuated from circular quay train  299995\n",
       "299996      companies invest billions in png gas industry  299996\n",
       "299997              company looks to expand basalt quarry  299997\n",
       "299998     cooloola mayor flags support for super council  299998\n",
       "299999              cosgrove relieved vcs staying in aust  299999\n",
       "\n",
       "[300000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [decid, communiti, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess all the headlines, saving the list of results as 'processed_docs'\n",
    "\n",
    "preprocessed_docs = documents['headline_text'].map(preprocessing_text)  \n",
    "\n",
    "preprocessed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n",
      "11 aust\n",
      "12 rise\n",
      "13 staff\n",
      "14 strike\n",
      "15 affect\n",
      "16 australian\n",
      "17 travel\n",
      "18 ambiti\n",
      "19 jump\n",
      "20 olsson\n",
      "21 tripl\n",
      "22 win\n",
      "23 antic\n",
      "24 barca\n",
      "25 break\n",
      "26 delight\n",
      "27 record\n",
      "28 aussi\n",
      "29 match\n",
      "30 memphi\n",
      "31 qualifi\n",
      "32 stosur\n",
      "33 wast\n",
      "34 address\n",
      "35 council\n",
      "36 iraq\n",
      "37 secur\n",
      "38 australia\n",
      "39 lock\n",
      "40 timet\n",
      "41 contribut\n",
      "42 million\n",
      "43 birthday\n",
      "44 celebr\n",
      "45 robson\n",
      "46 ahead\n",
      "47 bathhous\n",
      "48 plan\n",
      "49 championship\n",
      "50 cycl\n"
     ]
    }
   ],
   "source": [
    "#creating a dictionary using the gensim corpora dictionary for the preprocessed documents.\n",
    "dictionary = gensim.corpora.Dictionary(preprocessed_docs)\n",
    "\n",
    "count = 0\n",
    "for words, tags in dictionary.iteritems():\n",
    "    print(words, tags)\n",
    "    count = count + 1\n",
    "    if count > 50:\n",
    "        break;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(449, 1), (630, 1), (864, 1), (1074, 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###creating the BAG_of_word model dictionary to find the how man words and how many times a word appear.\n",
    "\n",
    "doc_corpus=[dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
    "\n",
    "doc_corpus[document_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.interfaces.TransformedCorpus object at 0x000001EA7726DE08>\n"
     ]
    }
   ],
   "source": [
    "###creating a tf-idf vectorizer and initializing the model\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf_model = models.TfidfModel(doc_corpus) \n",
    "\n",
    "##transforming the tf_idf models\n",
    "tfidf= tfidf_model[doc_corpus]\n",
    "print(tfidf[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5959919082495837),\n",
      " (1, 0.3920069955308767),\n",
      " (2, 0.48532280284497653),\n",
      " (3, 0.5055550788930631)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview TF-IDF scores for our first document --> --> (token_id, tfidf score)\n",
    "'''\n",
    "from pprint import pprint\n",
    "for doc in tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.LdaModel(doc_corpus, \n",
    "                                    num_topics = 10, \n",
    "                                    id2word = dictionary,                                    \n",
    "                                    passes = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0.024*\"help\" + 0.019*\"drought\" + 0.018*\"work\" + 0.018*\"govt\" + 0.017*\"farmer\" + 0.016*\"resid\" + 0.015*\"indigen\" + 0.014*\"offer\" + 0.014*\"return\" + 0.013*\"urg\" \n",
      "Words: 0\n",
      "\n",
      "\n",
      "Topic: 0.026*\"claim\" + 0.023*\"jail\" + 0.021*\"reject\" + 0.016*\"australian\" + 0.015*\"year\" + 0.015*\"aust\" + 0.014*\"market\" + 0.013*\"fear\" + 0.012*\"high\" + 0.011*\"timor\" \n",
      "Words: 1\n",
      "\n",
      "\n",
      "Topic: 0.026*\"open\" + 0.022*\"back\" + 0.020*\"world\" + 0.019*\"record\" + 0.016*\"teen\" + 0.015*\"expect\" + 0.015*\"play\" + 0.014*\"develop\" + 0.014*\"final\" + 0.013*\"look\" \n",
      "Words: 2\n",
      "\n",
      "\n",
      "Topic: 0.048*\"govt\" + 0.018*\"hospit\" + 0.017*\"chang\" + 0.017*\"health\" + 0.017*\"school\" + 0.017*\"call\" + 0.016*\"say\" + 0.016*\"servic\" + 0.016*\"minist\" + 0.015*\"fund\" \n",
      "Words: 3\n",
      "\n",
      "\n",
      "Topic: 0.069*\"polic\" + 0.024*\"crash\" + 0.023*\"investig\" + 0.018*\"road\" + 0.017*\"test\" + 0.016*\"elect\" + 0.014*\"driver\" + 0.013*\"mayor\" + 0.013*\"probe\" + 0.011*\"assault\" \n",
      "Words: 4\n",
      "\n",
      "\n",
      "Topic: 0.051*\"water\" + 0.037*\"council\" + 0.028*\"plan\" + 0.020*\"continu\" + 0.020*\"push\" + 0.018*\"nation\" + 0.017*\"miss\" + 0.015*\"boost\" + 0.015*\"consid\" + 0.015*\"meet\" \n",
      "Words: 5\n",
      "\n",
      "\n",
      "Topic: 0.031*\"say\" + 0.025*\"opposit\" + 0.024*\"forc\" + 0.024*\"power\" + 0.017*\"take\" + 0.015*\"time\" + 0.015*\"lead\" + 0.015*\"leader\" + 0.013*\"storm\" + 0.013*\"prompt\" \n",
      "Words: 6\n",
      "\n",
      "\n",
      "Topic: 0.040*\"urg\" + 0.023*\"hous\" + 0.023*\"labor\" + 0.020*\"worker\" + 0.019*\"death\" + 0.019*\"govt\" + 0.018*\"polic\" + 0.016*\"industri\" + 0.016*\"arrest\" + 0.015*\"public\" \n",
      "Words: 7\n",
      "\n",
      "\n",
      "Topic: 0.028*\"kill\" + 0.027*\"iraq\" + 0.017*\"howard\" + 0.015*\"deal\" + 0.015*\"troop\" + 0.013*\"chief\" + 0.012*\"sydney\" + 0.012*\"leav\" + 0.012*\"say\" + 0.011*\"announc\" \n",
      "Words: 8\n",
      "\n",
      "\n",
      "Topic: 0.044*\"charg\" + 0.037*\"court\" + 0.035*\"face\" + 0.024*\"closer\" + 0.024*\"accus\" + 0.022*\"murder\" + 0.020*\"attack\" + 0.017*\"blaze\" + 0.016*\"drug\" + 0.016*\"centr\" \n",
      "Words: 9\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for idx, topic in model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(topic, idx ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Running LDA model uisng tfidf model\n",
    "\n",
    "tf_idf_model = gensim.models.LdaModel(tfidf, \n",
    "                                       id2word = dictionary,\n",
    "                                       num_topics = 10,\n",
    "                                       passes = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic : 0 \n",
      "Words: 0.015*\"teen\" + 0.012*\"crew\" + 0.011*\"prompt\" + 0.011*\"open\" + 0.011*\"green\" + 0.009*\"research\" + 0.008*\"station\" + 0.008*\"remain\" + 0.008*\"futur\" + 0.007*\"deni\" \n",
      "\n",
      "\n",
      "Topic : 1 \n",
      "Words: 0.014*\"rudd\" + 0.014*\"troop\" + 0.011*\"leader\" + 0.010*\"market\" + 0.010*\"fire\" + 0.009*\"aust\" + 0.009*\"iraq\" + 0.009*\"nuclear\" + 0.009*\"solomon\" + 0.008*\"north\" \n",
      "\n",
      "\n",
      "Topic : 2 \n",
      "Words: 0.011*\"stand\" + 0.011*\"arrest\" + 0.010*\"climat\" + 0.010*\"sale\" + 0.009*\"bush\" + 0.009*\"condit\" + 0.009*\"qanta\" + 0.008*\"trade\" + 0.008*\"make\" + 0.008*\"howard\" \n",
      "\n",
      "\n",
      "Topic : 3 \n",
      "Words: 0.011*\"rescu\" + 0.010*\"rate\" + 0.010*\"train\" + 0.009*\"debat\" + 0.009*\"propos\" + 0.008*\"soldier\" + 0.008*\"central\" + 0.008*\"pledg\" + 0.008*\"fin\" + 0.008*\"japan\" \n",
      "\n",
      "\n",
      "Topic : 4 \n",
      "Words: 0.018*\"water\" + 0.014*\"govt\" + 0.011*\"plan\" + 0.011*\"hick\" + 0.010*\"council\" + 0.010*\"break\" + 0.009*\"hous\" + 0.008*\"fund\" + 0.008*\"communiti\" + 0.008*\"sydney\" \n",
      "\n",
      "\n",
      "Topic : 5 \n",
      "Words: 0.024*\"crash\" + 0.023*\"polic\" + 0.020*\"charg\" + 0.019*\"investig\" + 0.014*\"kill\" + 0.012*\"woman\" + 0.012*\"murder\" + 0.012*\"die\" + 0.010*\"court\" + 0.010*\"assault\" \n",
      "\n",
      "\n",
      "Topic : 6 \n",
      "Words: 0.013*\"england\" + 0.011*\"test\" + 0.010*\"guilti\" + 0.010*\"time\" + 0.010*\"cyclon\" + 0.009*\"gold\" + 0.009*\"storm\" + 0.008*\"coast\" + 0.008*\"oper\" + 0.008*\"near\" \n",
      "\n",
      "\n",
      "Topic : 7 \n",
      "Words: 0.038*\"closer\" + 0.012*\"miss\" + 0.011*\"search\" + 0.011*\"resid\" + 0.009*\"worker\" + 0.009*\"drought\" + 0.009*\"bushfir\" + 0.008*\"farm\" + 0.008*\"author\" + 0.008*\"rain\" \n",
      "\n",
      "\n",
      "Topic : 8 \n",
      "Words: 0.014*\"blaze\" + 0.011*\"firefight\" + 0.010*\"flood\" + 0.009*\"water\" + 0.009*\"campaign\" + 0.009*\"local\" + 0.009*\"east\" + 0.008*\"timor\" + 0.008*\"poll\" + 0.008*\"control\" \n",
      "\n",
      "\n",
      "Topic : 9 \n",
      "Words: 0.015*\"elect\" + 0.010*\"labor\" + 0.010*\"liber\" + 0.010*\"appeal\" + 0.009*\"tiger\" + 0.009*\"adelaid\" + 0.008*\"iemma\" + 0.008*\"emerg\" + 0.008*\"worri\" + 0.008*\"brisban\" \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in tf_idf_model.print_topics(-1):\n",
    "    print(\"Topic : {} \\nWords: {} \".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cost', 'blowout', 'forecast', 'water', 'treatment', 'plan']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_docs[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5273331999778748\t \n",
      "Topic: 0.024*\"help\" + 0.019*\"drought\" + 0.018*\"work\" + 0.018*\"govt\" + 0.017*\"farmer\" + 0.016*\"resid\" + 0.015*\"indigen\" + 0.014*\"offer\" + 0.014*\"return\" + 0.013*\"urg\"\n",
      "\n",
      "Score: 0.37264299392700195\t \n",
      "Topic: 0.051*\"water\" + 0.037*\"council\" + 0.028*\"plan\" + 0.020*\"continu\" + 0.020*\"push\" + 0.018*\"nation\" + 0.017*\"miss\" + 0.015*\"boost\" + 0.015*\"consid\" + 0.015*\"meet\"\n",
      "\n",
      "Score: 0.012507086619734764\t \n",
      "Topic: 0.048*\"govt\" + 0.018*\"hospit\" + 0.017*\"chang\" + 0.017*\"health\" + 0.017*\"school\" + 0.017*\"call\" + 0.016*\"say\" + 0.016*\"servic\" + 0.016*\"minist\" + 0.015*\"fund\"\n",
      "\n",
      "Score: 0.012503442354500294\t \n",
      "Topic: 0.040*\"urg\" + 0.023*\"hous\" + 0.023*\"labor\" + 0.020*\"worker\" + 0.019*\"death\" + 0.019*\"govt\" + 0.018*\"polic\" + 0.016*\"industri\" + 0.016*\"arrest\" + 0.015*\"public\"\n",
      "\n",
      "Score: 0.012502958066761494\t \n",
      "Topic: 0.031*\"say\" + 0.025*\"opposit\" + 0.024*\"forc\" + 0.024*\"power\" + 0.017*\"take\" + 0.015*\"time\" + 0.015*\"lead\" + 0.015*\"leader\" + 0.013*\"storm\" + 0.013*\"prompt\"\n",
      "\n",
      "Score: 0.012502354569733143\t \n",
      "Topic: 0.028*\"kill\" + 0.027*\"iraq\" + 0.017*\"howard\" + 0.015*\"deal\" + 0.015*\"troop\" + 0.013*\"chief\" + 0.012*\"sydney\" + 0.012*\"leav\" + 0.012*\"say\" + 0.011*\"announc\"\n",
      "\n",
      "Score: 0.012501992285251617\t \n",
      "Topic: 0.026*\"claim\" + 0.023*\"jail\" + 0.021*\"reject\" + 0.016*\"australian\" + 0.015*\"year\" + 0.015*\"aust\" + 0.014*\"market\" + 0.013*\"fear\" + 0.012*\"high\" + 0.011*\"timor\"\n",
      "\n",
      "Score: 0.012501992285251617\t \n",
      "Topic: 0.026*\"open\" + 0.022*\"back\" + 0.020*\"world\" + 0.019*\"record\" + 0.016*\"teen\" + 0.015*\"expect\" + 0.015*\"play\" + 0.014*\"develop\" + 0.014*\"final\" + 0.013*\"look\"\n",
      "\n",
      "Score: 0.012501992285251617\t \n",
      "Topic: 0.069*\"polic\" + 0.024*\"crash\" + 0.023*\"investig\" + 0.018*\"road\" + 0.017*\"test\" + 0.016*\"elect\" + 0.014*\"driver\" + 0.013*\"mayor\" + 0.013*\"probe\" + 0.011*\"assault\"\n",
      "\n",
      "Score: 0.012501992285251617\t \n",
      "Topic: 0.044*\"charg\" + 0.037*\"court\" + 0.035*\"face\" + 0.024*\"closer\" + 0.024*\"accus\" + 0.022*\"murder\" + 0.020*\"attack\" + 0.017*\"blaze\" + 0.016*\"drug\" + 0.016*\"centr\"\n"
     ]
    }
   ],
   "source": [
    "document_num = 4310\n",
    "# Our test document is document number 600\n",
    "\n",
    "\n",
    "# Our test document is document number 600\n",
    "for index, score in sorted(model[doc_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.887478768825531\t \n",
      "Topic: 0.014*\"blaze\" + 0.011*\"firefight\" + 0.010*\"flood\" + 0.009*\"water\" + 0.009*\"campaign\" + 0.009*\"local\" + 0.009*\"east\" + 0.008*\"timor\" + 0.008*\"poll\" + 0.008*\"control\"\n",
      "\n",
      "Score: 0.012503419071435928\t \n",
      "Topic: 0.018*\"water\" + 0.014*\"govt\" + 0.011*\"plan\" + 0.011*\"hick\" + 0.010*\"council\" + 0.010*\"break\" + 0.009*\"hous\" + 0.008*\"fund\" + 0.008*\"communiti\" + 0.008*\"sydney\"\n",
      "\n",
      "Score: 0.012503261677920818\t \n",
      "Topic: 0.014*\"rudd\" + 0.014*\"troop\" + 0.011*\"leader\" + 0.010*\"market\" + 0.010*\"fire\" + 0.009*\"aust\" + 0.009*\"iraq\" + 0.009*\"nuclear\" + 0.009*\"solomon\" + 0.008*\"north\"\n",
      "\n",
      "Score: 0.012502910569310188\t \n",
      "Topic: 0.011*\"stand\" + 0.011*\"arrest\" + 0.010*\"climat\" + 0.010*\"sale\" + 0.009*\"bush\" + 0.009*\"condit\" + 0.009*\"qanta\" + 0.008*\"trade\" + 0.008*\"make\" + 0.008*\"howard\"\n",
      "\n",
      "Score: 0.01250283233821392\t \n",
      "Topic: 0.038*\"closer\" + 0.012*\"miss\" + 0.011*\"search\" + 0.011*\"resid\" + 0.009*\"worker\" + 0.009*\"drought\" + 0.009*\"bushfir\" + 0.008*\"farm\" + 0.008*\"author\" + 0.008*\"rain\"\n",
      "\n",
      "Score: 0.01250260416418314\t \n",
      "Topic: 0.015*\"teen\" + 0.012*\"crew\" + 0.011*\"prompt\" + 0.011*\"open\" + 0.011*\"green\" + 0.009*\"research\" + 0.008*\"station\" + 0.008*\"remain\" + 0.008*\"futur\" + 0.007*\"deni\"\n",
      "\n",
      "Score: 0.012501809746026993\t \n",
      "Topic: 0.015*\"elect\" + 0.010*\"labor\" + 0.010*\"liber\" + 0.010*\"appeal\" + 0.009*\"tiger\" + 0.009*\"adelaid\" + 0.008*\"iemma\" + 0.008*\"emerg\" + 0.008*\"worri\" + 0.008*\"brisban\"\n",
      "\n",
      "Score: 0.012501788325607777\t \n",
      "Topic: 0.011*\"rescu\" + 0.010*\"rate\" + 0.010*\"train\" + 0.009*\"debat\" + 0.009*\"propos\" + 0.008*\"soldier\" + 0.008*\"central\" + 0.008*\"pledg\" + 0.008*\"fin\" + 0.008*\"japan\"\n",
      "\n",
      "Score: 0.012501743622124195\t \n",
      "Topic: 0.024*\"crash\" + 0.023*\"polic\" + 0.020*\"charg\" + 0.019*\"investig\" + 0.014*\"kill\" + 0.012*\"woman\" + 0.012*\"murder\" + 0.012*\"die\" + 0.010*\"court\" + 0.010*\"assault\"\n",
      "\n",
      "Score: 0.012500887736678123\t \n",
      "Topic: 0.013*\"england\" + 0.011*\"test\" + 0.010*\"guilti\" + 0.010*\"time\" + 0.010*\"cyclon\" + 0.009*\"gold\" + 0.009*\"storm\" + 0.008*\"coast\" + 0.008*\"oper\" + 0.008*\"near\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check which topic our test document belongs to using the LDA TF-IDF model.\n",
    "'''\n",
    "# Our test document is document number 4310\n",
    "for index, score in sorted(tf_idf_model[doc_corpus[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, tf_idf_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(399, 1), (1086, 1), (1832, 1), (3015, 1), (8850, 1)]\n",
      "Score: 0.6200794577598572\t Topic: 0.044*\"charg\" + 0.037*\"court\" + 0.035*\"face\" + 0.024*\"closer\" + 0.024*\"accus\"\n",
      "Score: 0.2199477255344391\t Topic: 0.048*\"govt\" + 0.018*\"hospit\" + 0.017*\"chang\" + 0.017*\"health\" + 0.017*\"school\"\n",
      "Score: 0.019996600225567818\t Topic: 0.024*\"help\" + 0.019*\"drought\" + 0.018*\"work\" + 0.018*\"govt\" + 0.017*\"farmer\"\n",
      "Score: 0.019996600225567818\t Topic: 0.026*\"claim\" + 0.023*\"jail\" + 0.021*\"reject\" + 0.016*\"australian\" + 0.015*\"year\"\n",
      "Score: 0.019996600225567818\t Topic: 0.026*\"open\" + 0.022*\"back\" + 0.020*\"world\" + 0.019*\"record\" + 0.016*\"teen\"\n",
      "Score: 0.019996600225567818\t Topic: 0.069*\"polic\" + 0.024*\"crash\" + 0.023*\"investig\" + 0.018*\"road\" + 0.017*\"test\"\n",
      "Score: 0.019996600225567818\t Topic: 0.051*\"water\" + 0.037*\"council\" + 0.028*\"plan\" + 0.020*\"continu\" + 0.020*\"push\"\n",
      "Score: 0.019996600225567818\t Topic: 0.031*\"say\" + 0.025*\"opposit\" + 0.024*\"forc\" + 0.024*\"power\" + 0.017*\"take\"\n",
      "Score: 0.019996600225567818\t Topic: 0.040*\"urg\" + 0.023*\"hous\" + 0.023*\"labor\" + 0.020*\"worker\" + 0.019*\"death\"\n",
      "Score: 0.019996600225567818\t Topic: 0.028*\"kill\" + 0.027*\"iraq\" + 0.017*\"howard\" + 0.015*\"deal\" + 0.015*\"troop\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = \"My favorite sports activities are running and swimming.\"\n",
    "\n",
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocessing_text(unseen_document))\n",
    "print(bow_vector)\n",
    "\n",
    "for index, score in sorted(model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, model.print_topic(index, 5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
